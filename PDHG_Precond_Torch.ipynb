{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c6970982-e9c8-4cd0-a0ca-543ff00ecf79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6970982-e9c8-4cd0-a0ca-543ff00ecf79",
        "outputId": "d2771ce7-8a5a-4636-a2db-55f0ac30c236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pulp\n",
            "  Downloading pulp-3.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting cplex\n",
            "  Downloading cplex-22.1.2.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n",
            "Downloading pulp-3.2.1-py3-none-any.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cplex-22.1.2.0-cp311-cp311-manylinux2014_x86_64.whl (44.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pulp, cplex\n",
            "Successfully installed cplex-22.1.2.0 pulp-3.2.1\n"
          ]
        }
      ],
      "source": [
        "pip install pulp cplex scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f85453f-5f8b-47cc-a60f-b499d6fb75ec",
      "metadata": {
        "id": "8f85453f-5f8b-47cc-a60f-b499d6fb75ec"
      },
      "source": [
        "The general form of LP problems we will consider is:\n",
        "\n",
        "        min_x  cᵀx\n",
        "   subject to  Gx ≥ h\n",
        "               Ax = b\n",
        "               l ≤ x ≤ u\n",
        "\n",
        "The benchmark datasets use the .mps format for LP problems so there is a function to extract the parameters from this file type into the form above. But since I haven't figured out how to access these datasets yet there is also a function to generate large feasible LP example problems in the .mps format. Finally, there is a preliminary function that implements the PDHG algorithm but not yet any of the enhancments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d6990fd1-e684-4a7e-b118-3b454955f0eb",
      "metadata": {
        "id": "d6990fd1-e684-4a7e-b118-3b454955f0eb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import random as sparse_random\n",
        "import pulp\n",
        "\n",
        "# This function generates large feasible LP problems to test and saves them in the .mps format\n",
        "def generate_feasible_lp(num_vars=100, num_ineq=200, num_eq=50, density=0.05, mps_filename=\"generated_lp.mps\"):\n",
        "\n",
        "    # The number of variables in each constraint with nonzero coefficients will be roughly density * num_vars\n",
        "\n",
        "    rng = np.random.default_rng(0)\n",
        "\n",
        "    # Step 1: Feasible solution\n",
        "    x_feas = rng.uniform(low=-10, high=10, size=(num_vars, 1))\n",
        "\n",
        "    # Step 2: Sparse matrices (convert to dense)\n",
        "    G_sparse = sparse_random(num_ineq, num_vars, density=density, format='csr', random_state=None)\n",
        "    A_sparse = sparse_random(num_eq, num_vars, density=density, format='csr', random_state=None)\n",
        "\n",
        "    G = G_sparse.toarray()\n",
        "    A = A_sparse.toarray()\n",
        "\n",
        "    # Step 3: RHS vectors\n",
        "    h = G @ x_feas + rng.uniform(0.1, 5.0, size=(num_ineq, 1))\n",
        "    b = A @ x_feas\n",
        "\n",
        "    # Step 4: Bounds\n",
        "    l = x_feas - rng.uniform(1, 5, size=(num_vars, 1))\n",
        "    u = x_feas + rng.uniform(1, 5, size=(num_vars, 1))\n",
        "    l = np.maximum(l, -1e4)\n",
        "    u = np.minimum(u, 1e4)\n",
        "\n",
        "    # Step 5: Objective\n",
        "    c = rng.normal(size=(num_vars, 1))\n",
        "\n",
        "    # Step 6: Write to MPS using pulp\n",
        "    prob = pulp.LpProblem(\"Feasible_LP\", pulp.LpMinimize)\n",
        "    x_vars = [\n",
        "        pulp.LpVariable(f\"x{i}\", lowBound=float(l[i]), upBound=float(u[i]))\n",
        "        for i in range(num_vars)\n",
        "    ]\n",
        "    prob += pulp.lpDot(c.flatten(), x_vars)\n",
        "\n",
        "    # Inequality constraints: Gx <= h\n",
        "    for i in range(num_ineq):\n",
        "        prob += pulp.lpDot(G[i], x_vars) <= float(h[i]), f\"ineq_{i}\"\n",
        "\n",
        "    # Equality constraints: Ax = b\n",
        "    for i in range(num_eq):\n",
        "        prob += pulp.lpDot(A[i], x_vars) == float(b[i]), f\"eq_{i}\"\n",
        "\n",
        "    prob.writeMPS(mps_filename)\n",
        "    print(f\"✅ LP written to: {mps_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6d2c3424-4a61-4caa-990e-f3f0a146dd00",
      "metadata": {
        "id": "6d2c3424-4a61-4caa-990e-f3f0a146dd00"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cplex\n",
        "from cplex.exceptions import CplexError\n",
        "\n",
        "# This function extracts the parameters of the LP from the .mps format to the general form\n",
        "def mps_to_standard_form(mps_file):\n",
        "    try:\n",
        "        cpx = cplex.Cplex(mps_file)\n",
        "        cpx.set_results_stream(None)  # mute output\n",
        "\n",
        "        # Number of variables and constraints\n",
        "        num_vars = cpx.variables.get_num()\n",
        "        num_constraints = cpx.linear_constraints.get_num()\n",
        "\n",
        "        # Objective vector (c)\n",
        "        c = np.array(cpx.objective.get_linear())\n",
        "\n",
        "        # Constraint matrix\n",
        "        A_full = cpx.linear_constraints.get_rows()\n",
        "        senses = cpx.linear_constraints.get_senses()\n",
        "        rhs = np.array(cpx.linear_constraints.get_rhs())\n",
        "\n",
        "        A = []\n",
        "        G = []\n",
        "        b = []\n",
        "        h = []\n",
        "\n",
        "        for i, (row, sense, rhs_i) in enumerate(zip(A_full, senses, rhs)):\n",
        "            row_vec = np.zeros(num_vars)\n",
        "            for idx, val in zip(row.ind, row.val):\n",
        "                row_vec[idx] = val\n",
        "            if sense == 'E':  # Equality constraint\n",
        "                A.append(row_vec)\n",
        "                b.append(rhs_i)\n",
        "            elif sense == 'G':  # Greater than or equal\n",
        "                G.append(row_vec)\n",
        "                h.append(rhs_i)\n",
        "            elif sense == 'L':  # Less than or equal\n",
        "                # convert to -Gx ≥ -h\n",
        "                G.append(-row_vec)\n",
        "                h.append(-rhs_i)\n",
        "\n",
        "        A = np.array(A) if A else np.zeros((0, num_vars))\n",
        "        b = np.array(b) if b else np.zeros(0)\n",
        "        G = np.array(G) if G else np.zeros((0, num_vars))\n",
        "        h = np.array(h) if h else np.zeros(0)\n",
        "\n",
        "        # Bounds\n",
        "        l = np.array(cpx.variables.get_lower_bounds())\n",
        "        u = np.array(cpx.variables.get_upper_bounds())\n",
        "\n",
        "        c = c.reshape(-1, 1)\n",
        "        h = h.reshape(-1, 1)\n",
        "        b = b.reshape(-1, 1)\n",
        "        l = l.reshape(-1, 1)\n",
        "        u = u.reshape(-1, 1)\n",
        "\n",
        "        return c, G, h, A, b, l, u\n",
        "\n",
        "    except CplexError as e:\n",
        "        print(\"CPLEX Error:\", e)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ac77ad94-b5b1-4e75-a60b-65732bf2f00d",
      "metadata": {
        "id": "ac77ad94-b5b1-4e75-a60b-65732bf2f00d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def proj_Lam(lam):\n",
        "    return lam\n",
        "\n",
        "def pdhg(c, G, h, A, b, l, u, max_iter=1000, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Solves:\n",
        "        min cᵀx s.t. Gx ≥ h, Ax = b, l ≤ x ≤ u\n",
        "    using PDHG algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define Parameters\n",
        "    K = np.vstack([G, A])\n",
        "    q = np.vstack([h, b])\n",
        "\n",
        "    eta = 0.9/np.linalg.norm(K, 2)\n",
        "    omega = 10\n",
        "\n",
        "    tau = eta/omega\n",
        "    sigma = eta*omega\n",
        "\n",
        "    m_1 = G.shape[0]\n",
        "    m_2 = A.shape[0]\n",
        "    n = c.shape[0]\n",
        "\n",
        "    # Termination Parameters\n",
        "    tol_prim = tol * (1 + np.linalg.norm(c))\n",
        "    tol_dual = tol * (1 + np.linalg.norm(q))\n",
        "\n",
        "    # Initialize Primal and Dual Variables\n",
        "    x = np.minimum(np.maximum(np.zeros((n, 1)), l), u)\n",
        "    y = np.zeros((m_1 + m_2, 1))\n",
        "\n",
        "    for i in range(max_iter):\n",
        "\n",
        "        # Primal update\n",
        "        x_old = x.copy()\n",
        "        # Project x onto box constraints l ≤ x ≤ u\n",
        "        grad = c - K.T @ y\n",
        "        x = np.minimum(np.maximum(x - tau * grad, l), u)\n",
        "\n",
        "        # Dual update\n",
        "        y += sigma * (q - K @ (2*x - x_old))\n",
        "        y[:m_1] = np.maximum(y[:m_1], 0)  # project onto constraint y:m_1 ≥ 0\n",
        "\n",
        "        # Check Termination Criteria Periodically\n",
        "        if i%1000 == 0:\n",
        "\n",
        "            dual_op = (q.T @ y)[0][0]\n",
        "            prim_op = (c.T @ x)[0][0]\n",
        "\n",
        "            lam_p_op = (l.T @ np.maximum(grad, 0))[0][0]\n",
        "            lam_n_op = (u.T @ np.minimum(grad, 0))[0][0]\n",
        "\n",
        "            print(dual_op + lam_p_op + lam_n_op, prim_op, i)\n",
        "\n",
        "            #print(np.linalg.norm(np.vstack([A @ x - b, np.maximum(h - G @ x, 0)])), np.linalg.norm(grad - proj_Lam(grad)), np.abs(dual_op + lam_p_op - lam_n_op - prim_op))\n",
        "            #print(tol_dual, tol_prim, tol * (1 + np.abs(dual_op + lam_p_op - lam_n_op) + np.abs(prim_op)))\n",
        "            if (\n",
        "                np.linalg.norm(np.vstack([A @ x - b, np.maximum(h - G @ x, 0)])) <= tol_dual\n",
        "                and np.linalg.norm(grad - proj_Lam(grad)) <= tol_prim\n",
        "                and np.abs(dual_op + lam_p_op + lam_n_op - prim_op) <= tol * (1 + np.abs(dual_op + lam_p_op + lam_n_op) + np.abs(prim_op))\n",
        "            ):\n",
        "                break\n",
        "\n",
        "    # Returns minimal objective value, minimizer estimate in list format, and the number of iterations run\n",
        "    return (c.T @ x)[0][0], x.T[0].tolist(), i + 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def ruiz_precondition_torch(K, c, b, h, l, u, max_iter=50, eps=1e-6):\n",
        "    \"\"\"\n",
        "    Applies Ruiz scaling preconditioning to linear programming problem matrices using PyTorch.\n",
        "\n",
        "    This method balances the constraint matrix and adjusts related vectors to improve\n",
        "    numerical stability and conditioning of the problem.\n",
        "\n",
        "    Args:\n",
        "        K: Constraint matrix (m x n) as NumPy array\n",
        "        c: Objective function coefficient vector (n x 1) as NumPy array\n",
        "        b: Equality constraint RHS vector (m_eq x 1) as NumPy array\n",
        "        h: Inequality constraint RHS vector (m_ineq x 1) as NumPy array\n",
        "        l: Lower bound vector (n x 1) as NumPy array\n",
        "        u: Upper bound vector (n x 1) as NumPy array\n",
        "        max_iter: Maximum number of scaling iterations (default: 50)\n",
        "        eps: Tolerance for considering a norm as zero (default: 1e-6)\n",
        "\n",
        "    Returns:\n",
        "        K_scaled: Scaled constraint matrix as NumPy array\n",
        "        c_tilde: Scaled objective coefficients as NumPy array\n",
        "        b_tilde: Scaled equality RHS as NumPy array\n",
        "        h_tilde: Scaled inequality RHS as NumPy array\n",
        "        l_tilde: Scaled lower bounds as NumPy array\n",
        "        u_tilde: Scaled upper bounds as NumPy array\n",
        "        D1: Row scaling factors (m x 1) as NumPy array\n",
        "        D2: Column scaling factors (n x 1) as NumPy array\n",
        "    \"\"\"\n",
        "    # Convert NumPy inputs to PyTorch tensors\n",
        "    K_tensor = torch.from_numpy(K).float()\n",
        "    c_tensor = torch.from_numpy(c).float() if c is not None else None\n",
        "    b_tensor = torch.from_numpy(b).float() if b is not None else None\n",
        "    h_tensor = torch.from_numpy(h).float() if h is not None else None\n",
        "    l_tensor = torch.from_numpy(l).float() if l is not None else None\n",
        "    u_tensor = torch.from_numpy(u).float() if u is not None else None\n",
        "\n",
        "    # Get problem dimensions\n",
        "    m, n = K_tensor.shape\n",
        "    m_ineq = h_tensor.shape[0] if h is not None else 0\n",
        "    m_eq = b_tensor.shape[0] if b is not None else 0\n",
        "\n",
        "    # Initialize scaling factors as identity\n",
        "    D1 = torch.ones(m, dtype=torch.float32)  # Row scaling factors\n",
        "    D2 = torch.ones(n, dtype=torch.float32)  # Column scaling factors\n",
        "    K_scaled = K_tensor.clone()  # Work on a copy of the constraint matrix\n",
        "\n",
        "    # Main Ruiz scaling loop\n",
        "    for it in range(max_iter):\n",
        "        # Step 1: Row scaling (balance row norms)\n",
        "        # Compute row-wise infinity norms (max absolute value in each row)\n",
        "        row_norms, _ = torch.max(torch.abs(K_scaled), dim=1)\n",
        "        # Replace near-zero norms with 1 to avoid division issues\n",
        "        row_norms[row_norms < eps] = 1.0\n",
        "        # Compute scaling factors as square roots of norms\n",
        "        row_scale = torch.sqrt(row_norms)\n",
        "        # Update cumulative row scaling factors\n",
        "        D1 /= row_scale\n",
        "        # Apply row scaling to matrix\n",
        "        K_scaled = K_scaled / row_scale.view(-1, 1)  # Broadcasting along rows\n",
        "\n",
        "        # Step 2: Column scaling (balance column norms)\n",
        "        # Compute column-wise infinity norms (max absolute value in each column)\n",
        "        col_norms, _ = torch.max(torch.abs(K_scaled), dim=0)\n",
        "        # Replace near-zero norms with 1 to avoid division issues\n",
        "        col_norms[col_norms < eps] = 1.0\n",
        "        # Compute scaling factors as square roots of norms\n",
        "        col_scale = torch.sqrt(col_norms)\n",
        "        # Update cumulative column scaling factors\n",
        "        D2 /= col_scale\n",
        "        # Apply column scaling to matrix\n",
        "        K_scaled = K_scaled / col_scale.view(1, -1)  # Broadcasting along columns\n",
        "\n",
        "        # Check convergence: stop if all row and column max norms are near 1\n",
        "        row_max, _ = torch.max(torch.abs(K_scaled), dim=1)\n",
        "        col_max, _ = torch.max(torch.abs(K_scaled), dim=0)\n",
        "        if (torch.max(torch.abs(1 - row_max)) < 0.1 and\n",
        "            torch.max(torch.abs(1 - col_max)) < 0.1):\n",
        "            break  # Exit early if convergence criteria met\n",
        "\n",
        "    # Convert scaling factors to column vectors for consistency\n",
        "    D1 = D1.view(-1, 1)\n",
        "    D2 = D2.view(-1, 1)\n",
        "\n",
        "    # Apply scaling to objective function coefficients\n",
        "    c_tilde = c_tensor * D2 if c is not None else None\n",
        "\n",
        "    # Apply scaling to variable bounds\n",
        "    l_tilde = l_tensor / D2 if l is not None else None\n",
        "    u_tilde = u_tensor / D2 if u is not None else None\n",
        "\n",
        "    # Apply scaling to constraint right-hand sides\n",
        "    if h is not None:\n",
        "        h_tilde = h_tensor * D1[:m_ineq]\n",
        "    else:\n",
        "        h_tilde = None\n",
        "\n",
        "    if b is not None:\n",
        "        b_tilde = b_tensor * D1[m_ineq:]\n",
        "    else:\n",
        "        b_tilde = None\n",
        "\n",
        "    # Convert all results back to NumPy arrays\n",
        "    K_scaled_np = K_scaled.numpy() if K_scaled is not None else None\n",
        "    c_tilde_np = c_tilde.numpy() if c_tilde is not None else None\n",
        "    b_tilde_np = b_tilde.numpy() if b_tilde is not None else None\n",
        "    h_tilde_np = h_tilde.numpy() if h_tilde is not None else None\n",
        "    l_tilde_np = l_tilde.numpy() if l_tilde is not None else None\n",
        "    u_tilde_np = u_tilde.numpy() if u_tilde is not None else None\n",
        "    D1_np = D1.numpy() if D1 is not None else None\n",
        "    D2_np = D2.numpy() if D2 is not None else None\n",
        "\n",
        "    # Return scaled problem components and scaling factors as NumPy arrays\n",
        "    return (K_scaled_np, c_tilde_np, b_tilde_np, h_tilde_np,\n",
        "            l_tilde_np, u_tilde_np, D1_np, D2_np)"
      ],
      "metadata": {
        "id": "U3KNtBj9woCz"
      },
      "id": "U3KNtBj9woCz",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def proj_Lam(lam):\n",
        "    \"\"\"Projection function for the dual variable (identity function in this context).\"\"\"\n",
        "    return lam\n",
        "\n",
        "def pdhg_precond(c, G, h, A, b, l, u, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Solves linear programming problem with PDHG algorithm using Ruiz preconditioning:\n",
        "        min cᵀx s.t. Gx ≥ h, Ax = b, l ≤ x ≤ u\n",
        "\n",
        "    The Ruiz preconditioning improves numerical conditioning by balancing the constraint matrix.\n",
        "\n",
        "    Args:\n",
        "        c: Objective coefficient vector (n x 1)\n",
        "        G: Inequality constraint matrix (m_ineq x n)\n",
        "        h: Inequality constraint RHS (m_ineq x 1)\n",
        "        A: Equality constraint matrix (m_eq x n)\n",
        "        b: Equality constraint RHS (m_eq x 1)\n",
        "        l: Lower bounds (n x 1)\n",
        "        u: Upper bounds (n x 1)\n",
        "        max_iter: Maximum iterations for PDHG\n",
        "        tol: Tolerance for termination criteria\n",
        "\n",
        "    Returns:\n",
        "        objective_value: Optimal objective value\n",
        "        solution: Minimizer estimate in list format\n",
        "        iterations: Number of iterations run\n",
        "    \"\"\"\n",
        "    # Save original parameters for solution recovery\n",
        "    c_orig = c.copy()  # Preserve original objective coefficients\n",
        "    G_orig = G.copy()  # Preserve original inequality matrix\n",
        "    h_orig = h.copy()  # Preserve original inequality RHS\n",
        "    A_orig = A.copy()  # Preserve original equality matrix\n",
        "    b_orig = b.copy()  # Preserve original equality RHS\n",
        "    l_orig = l.copy()  # Preserve original lower bounds\n",
        "    u_orig = u.copy()  # Preserve original upper bounds\n",
        "\n",
        "    # === Ruiz Preconditioning Section ===\n",
        "    # Combine all constraints into a single matrix\n",
        "    K_full = np.vstack([G, A])\n",
        "\n",
        "    # Apply Ruiz scaling to the problem\n",
        "    # This balances the constraint matrix and scales related parameters\n",
        "    K_scaled, c, b, h, l, u, D1, D2 = ruiz_precondition(\n",
        "        K_full, c, b, h, l, u\n",
        "    )\n",
        "\n",
        "    # Extract scaled constraint matrices\n",
        "    m_ineq = G.shape[0]  # Number of inequality constraints\n",
        "    G = K_scaled[:m_ineq]  # Scaled inequality constraint matrix\n",
        "    A = K_scaled[m_ineq:]  # Scaled equality constraint matrix\n",
        "\n",
        "    # === PDHG Algorithm with Scaled Parameters ===\n",
        "    # Define full constraint system for PDHG\n",
        "    K = K_scaled  # Combined scaled constraint matrix\n",
        "    q = np.vstack([h, b])  # Combined scaled constraint RHS\n",
        "\n",
        "    # Calculate step sizes based on scaled matrix norm\n",
        "    norm_K = np.linalg.norm(K, 2)  # Spectral norm of scaled matrix\n",
        "    eta = 0.9 / norm_K  # Scaling factor for step sizes\n",
        "    omega = 10  # Ratio parameter for primal/dual step sizes\n",
        "\n",
        "    tau = eta / omega  # Primal step size\n",
        "    sigma = eta * omega  # Dual step size\n",
        "\n",
        "    # Problem dimensions\n",
        "    m_1 = G.shape[0]  # Number of scaled inequality constraints\n",
        "    m_2 = A.shape[0]  # Number of scaled equality constraints\n",
        "    n = c.shape[0]  # Number of variables\n",
        "\n",
        "    # Adaptive termination tolerances\n",
        "    tol_prim = tol * (1 + np.linalg.norm(c))  # Primal feasibility tolerance\n",
        "    tol_dual = tol * (1 + np.linalg.norm(q))  # Dual feasibility tolerance\n",
        "\n",
        "    # Initialize primal and dual variables\n",
        "    x = np.minimum(np.maximum(np.zeros((n, 1)), l), u)  # Primal variable with box projection\n",
        "    y = np.zeros((m_1 + m_2, 1))  # Dual variable (Lagrange multipliers)\n",
        "\n",
        "    # Main PDHG iteration loop\n",
        "    for i in range(max_iter):\n",
        "        # Save previous primal iterate for extrapolation\n",
        "        x_old = x.copy()\n",
        "\n",
        "        # Primal update:\n",
        "        # 1. Compute gradient: c - Kᵀy\n",
        "        # 2. Take step in negative gradient direction: x - τ * gradient\n",
        "        # 3. Project onto box constraints [l, u]\n",
        "        grad = c - K.T @ y\n",
        "        x = np.minimum(np.maximum(x - tau * grad, l), u)\n",
        "\n",
        "        # Dual update:\n",
        "        # 1. Compute constraint violation: q - K(2x - x_old)\n",
        "        # 2. Take step in positive direction: y + σ * violation\n",
        "        # 3. Project inequality multipliers to non-negative orthant\n",
        "        y += sigma * (q - K @ (2*x - x_old))\n",
        "        y[:m_1] = np.maximum(y[:m_1], 0)  # Project inequality multipliers to ≥0\n",
        "\n",
        "        # Check termination criteria periodically\n",
        "        if i % 1000 == 0:\n",
        "            # Calculate dual objective value\n",
        "            dual_op = (q.T @ y)[0][0]\n",
        "\n",
        "            # Calculate primal objective value\n",
        "            prim_op = (c.T @ x)[0][0]\n",
        "\n",
        "            # Calculate complementarity terms for box constraints\n",
        "            lam_p_op = (l.T @ np.maximum(grad, 0))[0][0]  # Lower bound complementarity\n",
        "            lam_n_op = (u.T @ np.minimum(grad, 0))[0][0]  # Upper bound complementarity\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Duality gap: {dual_op + lam_p_op + lam_n_op - prim_op:.6f}, \"\n",
        "                  f\"Primal obj: {prim_op:.6f}, Iter: {i}\")\n",
        "\n",
        "            # Check termination criteria:\n",
        "            # 1. Primal feasibility: ||Ax - b|| + ||[Gx - h]_+||\n",
        "            primal_feas = np.linalg.norm(np.vstack([A @ x - b, np.maximum(h - G @ x, 0)]))\n",
        "\n",
        "            # 2. Dual feasibility: distance to projected gradient\n",
        "            dual_feas = np.linalg.norm(grad - proj_Lam(grad))\n",
        "\n",
        "            # 3. Duality gap: |dual_obj + comp_terms - primal_obj|\n",
        "            gap = np.abs(dual_op + lam_p_op + lam_n_op - prim_op)\n",
        "\n",
        "            # Scale tolerances by problem magnitude\n",
        "            scaled_tol = tol * (1 + np.abs(dual_op + lam_p_op + lam_n_op) + np.abs(prim_op))\n",
        "\n",
        "            if (primal_feas <= tol_dual and\n",
        "                dual_feas <= tol_prim and\n",
        "                gap <= scaled_tol):\n",
        "                print(f\"Converged at iteration {i}\")\n",
        "                break\n",
        "\n",
        "    # === Solution Recovery ===\n",
        "    # Transform solution back to original space: x_original = D2 * x_scaled\n",
        "    x_original = x * D2\n",
        "\n",
        "    # Calculate objective value using original parameters\n",
        "    objective_value = (c_orig.T @ x_original)[0][0]\n",
        "\n",
        "    # Return results\n",
        "    return objective_value, x_original.T[0].tolist(), i + 1"
      ],
      "metadata": {
        "id": "kIr7PyeFwqQx"
      },
      "id": "kIr7PyeFwqQx",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6fe8b000-1f6c-417c-9da3-1f6d921cccbc",
      "metadata": {
        "id": "6fe8b000-1f6c-417c-9da3-1f6d921cccbc"
      },
      "source": [
        "Testing the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "a4278777-d23b-4002-8059-03db2d2c20b7",
      "metadata": {
        "id": "a4278777-d23b-4002-8059-03db2d2c20b7"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "\n",
        "# Makes a timer to measure code omtimality\n",
        "class Timer:\n",
        "    def __enter__(self):\n",
        "        self.start = perf_counter()\n",
        "        return self\n",
        "    def __exit__(self, *args):\n",
        "        self.end = perf_counter()\n",
        "        self.elapsed = self.end - self.start\n",
        "        print(f\"Elapsed time: {self.elapsed:.6f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "cbd5f035-9038-4cbe-a797-c4fd45ffac7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbd5f035-9038-4cbe-a797-c4fd45ffac7a",
        "outputId": "cb7afe2c-2765-4caf-f317-53a745414256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LP written to: large_example.mps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2499066835.py:38: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  pulp.LpVariable(f\"x{i}\", lowBound=float(l[i]), upBound=float(u[i]))\n",
            "/tmp/ipython-input-5-2499066835.py:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  prob += pulp.lpDot(G[i], x_vars) <= float(h[i]), f\"ineq_{i}\"\n",
            "/tmp/ipython-input-5-2499066835.py:49: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  prob += pulp.lpDot(A[i], x_vars) == float(b[i]), f\"eq_{i}\"\n"
          ]
        }
      ],
      "source": [
        "# Create a feasible LP problem and save to a\n",
        "generate_feasible_lp(num_vars=6, num_ineq=4, num_eq=4, density=0.95, mps_filename=\"large_example.mps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "ebf3cab3-6d82-4eb4-a405-01fa4d7bdacf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebf3cab3-6d82-4eb4-a405-01fa4d7bdacf",
        "outputId": "9eef4b23-f5fe-4fd8-ebdd-48bac48951dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected objective sense:  MINIMIZE\n",
            "Selected objective  name:  OBJ\n",
            "Selected RHS        name:  RHS\n",
            "Selected bound      name:  BND\n",
            "Version identifier: 22.1.2.0 | 2024-12-10 | f4cec290b\n",
            "CPXPARAM_Read_DataCheck                          1\n",
            "Tried aggregator 1 time.\n",
            "No LP presolve or aggregator reductions.\n",
            "Presolve time = 0.00 sec. (0.01 ticks)\n",
            "\n",
            "Iteration log . . .\n",
            "Iteration:     1   Dual objective     =           -32.367428\n",
            "Elapsed time: 0.013115 seconds\n",
            "Objective value: -28.333108832267087\n",
            "Minimizer: xᵀ = [3.371809324017562, -5.615219725403, -10.261794339408421, -10.80378959065, 6.534395863185828, 9.73300777284794]\n"
          ]
        }
      ],
      "source": [
        "import cplex\n",
        "\n",
        "# Solve the LP using either primal simplex, dual simplex, or barrier (interior point).\n",
        "# Only works for LP with no more than 1000 constraints and no more than 1000 variables\n",
        "cpx = cplex.Cplex(\"large_example.mps\")\n",
        "\n",
        "# Times how long it takes to solve\n",
        "with Timer():\n",
        "    cpx.solve()\n",
        "\n",
        "# Save the minimizer and minimal objective values for comparison\n",
        "cpx_obj_val = cpx.solution.get_objective_value()\n",
        "cpx_min = cpx.solution.get_values()\n",
        "print(\"Objective value:\", cpx_obj_val)\n",
        "print(\"Minimizer: xᵀ =\", cpx_min)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract LP parameters from generated example\n",
        "with Timer():\n",
        "    c, G, h, A, b, l, u = mps_to_standard_form(\"large_example.mps\")\n",
        "    pdhg_obj_val, pdhg_min, iterations = pdhg(c, G, h, A, b, l, u, max_iter=1000000, tol=1e-8)\n",
        "print(\"Objective Value:\", pdhg_obj_val)\n",
        "print(\"Iterations:\", iterations)\n",
        "print(\"Minimizer: xᵀ =\",pdhg_min)\n",
        "\n",
        "# The distance between the two minimizer solutions\n",
        "# Should be small but won't be incredibly small since the vectors are high dimensional\n",
        "#distance = np.linalg.norm(np.array(pdhg_min) - np.array(cpx_min))\n",
        "#print(\"Distance:\", distance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm77RX7exTGH",
        "outputId": "81b59bfe-c6ea-4e9b-bfcd-8ecddf948fe3"
      },
      "id": "Wm77RX7exTGH",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected objective sense:  MINIMIZE\n",
            "Selected objective  name:  OBJ\n",
            "Selected RHS        name:  RHS\n",
            "Selected bound      name:  BND\n",
            "-33.36182673817835 -15.326473404286066 0\n",
            "-28.43007752369969 -28.347757469902128 1000\n",
            "-28.37302846154144 -28.33843298306913 2000\n",
            "-28.3493746732931 -28.335033772597825 3000\n",
            "-28.33968272549098 -28.33380061040593 4000\n",
            "-28.335746701745762 -28.333355779371473 5000\n",
            "-28.334160564696504 -28.33319632181417 6000\n",
            "-28.333525746560458 -28.333139561135084 7000\n",
            "-28.333273229197374 -28.333119516734747 8000\n",
            "-28.333173341359128 -28.3331125029304 9000\n",
            "-28.333134030538737 -28.333110075007912 10000\n",
            "-28.333118633009644 -28.333109245348645 11000\n",
            "-28.333112628733055 -28.333108966322676 12000\n",
            "-28.333110297166282 -28.33310887436651 13000\n",
            "-28.333109395389943 -28.333108844866153 14000\n",
            "Elapsed time: 0.401561 seconds\n",
            "Objective Value: -28.333108844866153\n",
            "Iterations: 14001\n",
            "Minimizer: xᵀ = [3.371809328851481, -5.615219725403, -10.261794346691847, -10.80378959065, 6.534395858545225, 9.733007779632489]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "966b5f25-ac4d-4d65-98fd-840774dfd404",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "966b5f25-ac4d-4d65-98fd-840774dfd404",
        "outputId": "3acfbfa4-21fb-4ce2-c2d1-dccfc4da8d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected objective sense:  MINIMIZE\n",
            "Selected objective  name:  OBJ\n",
            "Selected RHS        name:  RHS\n",
            "Selected bound      name:  BND\n",
            "Duality gap: -20.474026, Primal obj: -15.323857, Iter: 0\n",
            "Duality gap: -0.021258, Primal obj: -28.322230, Iter: 1000\n",
            "Duality gap: -0.034707, Primal obj: -28.331762, Iter: 2000\n",
            "Duality gap: -0.008414, Primal obj: -28.333674, Iter: 3000\n",
            "Duality gap: -0.000033, Primal obj: -28.333396, Iter: 4000\n",
            "Duality gap: -0.000739, Primal obj: -28.333144, Iter: 5000\n",
            "Duality gap: -0.000216, Primal obj: -28.333094, Iter: 6000\n",
            "Duality gap: -0.000016, Primal obj: -28.333101, Iter: 7000\n",
            "Duality gap: -0.000024, Primal obj: -28.333108, Iter: 8000\n",
            "Duality gap: -0.000006, Primal obj: -28.333109, Iter: 9000\n",
            "Duality gap: -0.000000, Primal obj: -28.333109, Iter: 10000\n",
            "Converged at iteration 10000\n",
            "Elapsed time: 0.161055 seconds\n",
            "Objective Value: -28.33310903185921\n",
            "Iterations: 10001\n",
            "Minimizer: xᵀ = [3.3718093935082307, -5.615219725403, -10.261794450448063, -10.80378959065, 6.5343957880993155, 9.733007890966695]\n"
          ]
        }
      ],
      "source": [
        "# Extract LP parameters from generated example\n",
        "with Timer():\n",
        "  c, G, h, A, b, l, u = mps_to_standard_form(\"large_example.mps\")\n",
        "  pdhg_obj_val, pdhg_min, iterations = pdhg_precond(c, G, h, A, b, l, u, max_iter=1000000, tol=1e-8)\n",
        "print(\"Objective Value:\", pdhg_obj_val)\n",
        "print(\"Iterations:\", iterations)\n",
        "print(\"Minimizer: xᵀ =\",pdhg_min)\n",
        "\n",
        "# The distance between the two minimizer solutions\n",
        "# Should be small but won't be incredibly small since the vectors are high dimensional\n",
        "#distance = np.linalg.norm(np.array(pdhg_min) - np.array(cpx_min))\n",
        "#print(\"Distance:\", distance)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}